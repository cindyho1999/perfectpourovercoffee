{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cindy Ho 127008544 Saturation Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders #this is a python library that divides a folder into testing, training, and validating data\n",
    "import tensorflow as tf #library for ML\n",
    "from tensorflow import keras #library to import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator #keras library for importting images\n",
    "from keras.models import Sequential #CNN model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D #CNN different layers needed\n",
    "import numpy as np #for plotting \n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "from sklearn.metrics import classification_report, confusion_matrix #used to view results of CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into Train/Test/Validate Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 7179 files [01:15, 95.57 files/s] \n"
     ]
    }
   ],
   "source": [
    "#library call: call folder where it receives inputs, then call folder where it outputs folder where it's all split up\n",
    "#seed 1337 means the amount of time it randomizes, seed means it traces how it randomizes & allows it to be reproducible\n",
    "#ratios - how it divides the data 80% in train, 10% in validate, and last 10% in validation\n",
    "#group_prefix - allows user to group the folders by filename prefix -> not necessary\n",
    "\n",
    "splitfolders.ratio(r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\TD', output=r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\TDoutput2',\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataGenerator implements functions useful for input image scaling and augmentation -- you may want more!\n",
    "#rescales divides the arrays by 255 (rgb values) to scale it down to 0 and 1\n",
    "#purpose is to reduce memory when inputting into CNN\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) #rescales training dataset\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255) #rescales validation dataset\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #rescales testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5728 images belonging to 11 classes.\n",
      "Found 713 images belonging to 11 classes.\n",
      "Found 727 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\TDoutput2\\train', #image path name\n",
    "        target_size=(32, 32), #rescales images from 640 x 480 pixels to 32 x 32 to conserve memory\n",
    "        color_mode='rgb', #image is RGB (red, green blue scale)\n",
    "        batch_size=1, #only 1 batch is inputted to train model\n",
    "        class_mode='categorical', #images are grouped into folders from 0-20 in categories therefore its categorical classes\n",
    "        shuffle=True, #turns on shuffle and shuffles the images\n",
    "        seed=1953) #amount of time it randomizes, seed means it traces how it randomizes & allows it to be reproducible\n",
    "\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\TDoutput2\\val',\n",
    "        target_size=(32, 32), #rescales images from 640 x 480 pixels to 32 x 32 to conserve memory\n",
    "        color_mode='rgb', #image is RGB (red, green blue scale)\n",
    "        batch_size=1, #only 1 batch is inputted to train model\n",
    "        class_mode='categorical', #images are grouped into folders from 0-20 in categories therefore its categorical classes\n",
    "        shuffle=False, #turns on shuffle and shuffles the images\n",
    "        seed=1953) #amount of time it randomizes, seed means it traces how it randomizes & allows it to be reproducible\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\TDoutput2\\test',\n",
    "    target_size=(32, 32),  #rescales images from 640 x 480 pixels to 32 x 32 to conserve memory\n",
    "    color_mode='rgb', #image is RGB (red, green blue scale)\n",
    "    batch_size=1, #only 1 batch is inputted to train model\n",
    "    class_mode='categorical', #images are grouped into folders from 0-20 in categories therefore its categorical classes\n",
    "    shuffle=False, #turns on shuffle and shuffles the images\n",
    "    seed=1953) #amount of time it randomizes, seed means it traces how it randomizes & allows it to be reproducible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# input layer (1) \n",
    "#filters -> number of filters model will learn = 16 \n",
    "# kernel size = 2-tuple specifying the width and height of the 2D convolution window (3x3)\n",
    "# padding = same to preserve the spatial dimensions of the volume for output volume size to match the input volume size\n",
    "# activiation function used relu -> best loss functions\n",
    "# input shape = number of images, pixel x pixel value, number of layers\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same', activation='relu', input_shape=(32, 32, 3))) \n",
    "\n",
    "# max pooling filter to reduce the dimensions of the feature maps by 2x2 (2)\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "# conv2D layer (4)\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same', activation='relu', input_shape=(32, 32, 3))) \n",
    "\n",
    "#max pooling filter to reduce the dimensions of the feature maps by 2x2 (5)\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "# flatten layer - to Flattening a tensor means to remove all of the dimensions except for one (7)\n",
    "model.add(Flatten())  \n",
    "\n",
    "# dense layer feeds all outputs from the previous layer to all its neurons,\n",
    "# each neuron providing one output to the next layer (8)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# dense layer repeats previous dense layer but outputs it to 32 instead (9)\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# output layer (10)\n",
    "#11 = number of classes it can be sorted to, linear regression model so use linear\n",
    "model.add(Dense(11, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                16400     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,075\n",
      "Trainable params: 20,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compiles the model utilizing Adam learning rate of 1e-4 & loss function as mean squared log error\n",
    "# attempted using mean square error, mean abs error, & sparse categorical but best was msle\n",
    "# accuracy was not used since image regression model focuses only on loss functions\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss='msle')  \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss='categorical_crossentropy')  \n",
    "\n",
    "#provides summary of layers within the cnn model, # of parameters inputted, & output shape\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cindy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5728/5728 [==============================] - 66s 11ms/step - loss: 2.3742 - val_loss: 2.3373\n",
      "Epoch 2/20\n",
      "5728/5728 [==============================] - 59s 10ms/step - loss: 2.3005 - val_loss: 2.2626\n",
      "Epoch 3/20\n",
      "5728/5728 [==============================] - 61s 11ms/step - loss: 2.2112 - val_loss: 2.1596\n",
      "Epoch 4/20\n",
      "5728/5728 [==============================] - 60s 10ms/step - loss: 2.0994 - val_loss: 2.0633\n",
      "Epoch 5/20\n",
      "5728/5728 [==============================] - 60s 10ms/step - loss: 1.9876 - val_loss: 1.9512\n",
      "Epoch 6/20\n",
      "5728/5728 [==============================] - 62s 11ms/step - loss: 1.8683 - val_loss: 1.8519\n",
      "Epoch 7/20\n",
      "5728/5728 [==============================] - 59s 10ms/step - loss: 1.7605 - val_loss: 1.7612\n",
      "Epoch 8/20\n",
      "5728/5728 [==============================] - 62s 11ms/step - loss: 1.6517 - val_loss: 1.6325\n",
      "Epoch 9/20\n",
      "5728/5728 [==============================] - 60s 11ms/step - loss: 1.5439 - val_loss: 1.5561\n",
      "Epoch 10/20\n",
      "5728/5728 [==============================] - 65s 11ms/step - loss: 1.4379 - val_loss: 1.4144\n",
      "Epoch 11/20\n",
      "5728/5728 [==============================] - 64s 11ms/step - loss: 1.3383 - val_loss: 1.3625\n",
      "Epoch 12/20\n",
      "5728/5728 [==============================] - 67s 12ms/step - loss: 1.2554 - val_loss: 1.2892\n",
      "Epoch 13/20\n",
      "5728/5728 [==============================] - 86s 15ms/step - loss: 1.1789 - val_loss: 1.2294\n",
      "Epoch 14/20\n",
      "5728/5728 [==============================] - 129s 23ms/step - loss: 1.1064 - val_loss: 1.2223\n",
      "Epoch 15/20\n",
      "5728/5728 [==============================] - 89s 16ms/step - loss: 1.0505 - val_loss: 1.1482\n",
      "Epoch 16/20\n",
      "5728/5728 [==============================] - 69s 12ms/step - loss: 0.9921 - val_loss: 1.0666\n",
      "Epoch 17/20\n",
      "5728/5728 [==============================] - 65s 11ms/step - loss: 0.9447 - val_loss: 1.0719\n",
      "Epoch 18/20\n",
      "5728/5728 [==============================] - 133s 23ms/step - loss: 0.8905 - val_loss: 1.0076\n",
      "Epoch 19/20\n",
      "5728/5728 [==============================] - 142s 25ms/step - loss: 0.8478 - val_loss: 0.9942\n",
      "Epoch 20/20\n",
      "5728/5728 [==============================] - 133s 23ms/step - loss: 0.8096 - val_loss: 0.9506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd7d5f6198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is to train the neural network model\n",
    "# determines steps per epoch for training = the number of batches to be selected for one epoch \n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size \n",
    "\n",
    "# determines steps per epoch for validating = the number of batches to be selected for one epoch \n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "model.fit_generator(generator=train_generator, #takes train generator function from above & has model fit it to training data\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN, #pulls steps per epoch value above for training\n",
    "                    validation_data=valid_generator, #takes validate generator function from above & has model test val data\n",
    "                    validation_steps=STEP_SIZE_VALID, #pulls steps per epoch value above for validating\n",
    "                    epochs=20 # epochs may be increased but best loss was at 20 epochs\n",
    "         ) \n",
    "#train different epochs & show side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[28 16  0  0  7  0  0  1  2  0  5]\n",
      " [ 1 50  1  0  4  1  2  2  6  1  0]\n",
      " [ 0  0 42  0  0  1  2  1  0  2  6]\n",
      " [ 0  2  0 51  2  6  3  2  3  0  1]\n",
      " [ 1  1  1  0 39  1  6  5  1  1  4]\n",
      " [ 0  2  2  0  1 58  4  1  2  0  3]\n",
      " [ 0  1  1  0  2  3 44  7  0  1  2]\n",
      " [ 0  0  3  4  2  4  4 33  2  7  2]\n",
      " [ 0  1  6  0  0  4  2  3 37  3  6]\n",
      " [ 0  2  3  2  0  2  1  7  1 40  4]\n",
      " [ 0  0  4  0  2  3  1  2  1  1 69]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.47      0.63        59\n",
      "           1       0.67      0.74      0.70        68\n",
      "          10       0.67      0.78      0.72        54\n",
      "           2       0.89      0.73      0.80        70\n",
      "           3       0.66      0.65      0.66        60\n",
      "           4       0.70      0.79      0.74        73\n",
      "           5       0.64      0.72      0.68        61\n",
      "           6       0.52      0.54      0.53        61\n",
      "           7       0.67      0.60      0.63        62\n",
      "           8       0.71      0.65      0.68        62\n",
      "           9       0.68      0.83      0.75        83\n",
      "\n",
      "    accuracy                           0.69       713\n",
      "   macro avg       0.70      0.68      0.68       713\n",
      "weighted avg       0.70      0.69      0.69       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the model predicting with the test image sets\n",
    "#model predicts with validate data & input number of steps needed per epoch\n",
    "Y_pred = model.predict(valid_generator, steps=STEP_SIZE_VALID)\n",
    "\n",
    "#Returns the indices of the maximum values along an axis to determine number of rows to correspond with number of classes\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#generates the confusion matrix to view results of cnn\n",
    "#inputs= Ground truth (correct) target values & y pred = estimated targets as returned by a classifier.\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(valid_generator.classes, y_pred))\n",
    "\n",
    "#generates classification report of precision, recall, f1 score, & support\n",
    "print('Classification Report')\n",
    "#inputs= Ground truth (correct) target values, y pred = estimated targets as returned by a classifier, & class labels\n",
    "print(classification_report(valid_generator.classes, y_pred, target_names=valid_generator.class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[29 18  0  0  3  1  2  1  6  0  0]\n",
      " [ 1 50  4  1  6  1  0  1  2  2  1]\n",
      " [ 0  0 35  0  0  3  3  4  3  1  6]\n",
      " [ 0  0  0 58  0  6  0  3  1  2  1]\n",
      " [ 0  0  1  1 42  2  5  6  0  0  4]\n",
      " [ 0  0  4  0  1 59  5  4  1  0  1]\n",
      " [ 0  0  4  1  2  2 47  5  0  0  2]\n",
      " [ 0  0  4  0  2  4  2 37  4  7  3]\n",
      " [ 0  1  3  0  1  2  2  4 41  4  5]\n",
      " [ 0  0  7  0  1  2  0  5  2 44  2]\n",
      " [ 0  2  1  0  0  2  0  0  1  1 77]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.48      0.64        60\n",
      "           1       0.70      0.72      0.71        69\n",
      "          10       0.56      0.64      0.59        55\n",
      "           2       0.95      0.82      0.88        71\n",
      "           3       0.72      0.69      0.71        61\n",
      "           4       0.70      0.79      0.74        75\n",
      "           5       0.71      0.75      0.73        63\n",
      "           6       0.53      0.59      0.56        63\n",
      "           7       0.67      0.65      0.66        63\n",
      "           8       0.72      0.70      0.71        63\n",
      "           9       0.75      0.92      0.83        84\n",
      "\n",
      "    accuracy                           0.71       727\n",
      "   macro avg       0.73      0.70      0.71       727\n",
      "weighted avg       0.73      0.71      0.71       727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the model predicting with the test image sets\n",
    "# determines steps per epoch for testing = the number of batches to be selected for one epoch \n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "\n",
    "#model predicts with test data & input number of steps needed per epoch\n",
    "Y_pred = model.predict(test_generator, steps=STEP_SIZE_TEST)\n",
    "\n",
    "#Returns the indices of the maximum values along an axis to determine number of rows to correspond with number of classes\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#generates the confusion matrix to view results of cnn\n",
    "#inputs= Ground truth (correct) target values & y pred = estimated targets as returned by a classifier.\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "#generates classification report of precision, recall, f1 score, & support\n",
    "print('Classification Report')\n",
    "#inputs= Ground truth (correct) target values, y pred = estimated targets as returned by a classifier, & class labels\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=train_generator.class_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1995 images belonging to 11 classes.\n",
      "Confusion Matrix\n",
      "[[ 55   9   6  23  38   1  12   0  14   6   7]\n",
      " [ 15   0   0  19   9  87  15   3   8   8  12]\n",
      " [ 31   7   2  53   8  17  12   2  15  11  18]\n",
      " [ 44  14   2  13  23  47   8   1   4  12   8]\n",
      " [  1   3   4  20  10 106  18   8  17   1   4]\n",
      " [ 11  19   0  42   4  38   0   0  58   0   4]\n",
      " [ 28  18   0  12  28  26  50   2  12   7   1]\n",
      " [ 30  32   2  16  10  58  17   1   9   1  16]\n",
      " [  5   0  25  28   8  27  12  11  42  24   2]\n",
      " [  9   3   1  14  70  30  22   1  22   1  11]\n",
      " [ 19  15  11  29  33  38   2  11   9   1  16]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.32      0.26       171\n",
      "           1       0.00      0.00      0.00       176\n",
      "          10       0.04      0.01      0.02       176\n",
      "           2       0.05      0.07      0.06       176\n",
      "           3       0.04      0.05      0.05       192\n",
      "           4       0.08      0.22      0.12       176\n",
      "           5       0.30      0.27      0.28       184\n",
      "           6       0.03      0.01      0.01       192\n",
      "           7       0.20      0.23      0.21       184\n",
      "           8       0.01      0.01      0.01       184\n",
      "           9       0.16      0.09      0.11       184\n",
      "\n",
      "    accuracy                           0.11      1995\n",
      "   macro avg       0.10      0.12      0.10      1995\n",
      "weighted avg       0.10      0.11      0.10      1995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_datagen = ImageDataGenerator(rescale=1./255) #rescales testing dataset\n",
    "\n",
    "demo_generator = demo_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\newtestimages\\set2',\n",
    "    target_size=(32, 32),  #rescales images from 640 x 480 pixels to 32 x 32 to conserve memory\n",
    "    color_mode='rgb', #image is RGB (red, green blue scale)\n",
    "    batch_size=1, #only 1 batch is inputted to train model\n",
    "    class_mode='categorical', #images are grouped into folders from 0-20 in categories therefore its categorical classes\n",
    "    shuffle=False, #turns on shuffle and shuffles the images\n",
    "    seed=1953) #amount of time it randomizes, seed means it traces how it randomizes & allows it to be reproducible\n",
    "\n",
    "#this is the model predicting with the test image sets\n",
    "# determines steps per epoch for testing = the number of batches to be selected for one epoch \n",
    "STEP_SIZE_DEMO = demo_generator.n//demo_generator.batch_size\n",
    "\n",
    "#model predicts with test data & input number of steps needed per epoch\n",
    "Y_pred = model.predict(demo_generator, steps=STEP_SIZE_DEMO)\n",
    "\n",
    "#Returns the indices of the maximum values along an axis to determine number of rows to correspond with number of classes\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#generates the confusion matrix to view results of cnn\n",
    "#inputs= Ground truth (correct) target values & y pred = estimated targets as returned by a classifier.\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(demo_generator.classes, y_pred))\n",
    "\n",
    "#generates classification report of precision, recall, f1 score, & support\n",
    "print('Classification Report')\n",
    "#inputs= Ground truth (correct) target values, y pred = estimated targets as returned by a classifier, & class labels\n",
    "print(classification_report(demo_generator.classes, y_pred, target_names=train_generator.class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "#opens my csv of labels and reads it into a list of labels\n",
    "with open(r\"C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\singlelabel.csv\", newline='') as csvfile:\n",
    "    labels = list(csv.reader(csvfile))\n",
    "\n",
    "size = (32,32) #size that I want my image resized to \n",
    "    \n",
    "imgs = [] #list that will hold my images\n",
    "path =  r\"C:\\Users\\cindy\\OneDrive\\Documents\\403\\inputimages\\single\" #path of images folder\n",
    "valid_images = [\".png\"] #extension of images\n",
    "for f in os.listdir(path): #iterate through every image in folder\n",
    "    ext = os.path.splitext(f)[1] #adding to list\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    imgs.append(np.array(Image.open(os.path.join(path,f)).resize(size,resample=0, box=None))) #opening image & adding path to it then appending it to list\n",
    "\n",
    "for i in range(len(imgs)): #iterates through my images list and scales it by 255 (RGB)\n",
    "    imgs[i] = (imgs[i].astype(float) / 255.0) #need to preprocess data, need to scale pixels to be between 0-1\n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00169356 0.00590914 0.23452011 0.04973525 0.05594357 0.11450691\n",
      "  0.01946886 0.03298127 0.02917369 0.05005271 0.4060149 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(np.array(imgs)) #prediction on my single image\n",
    "print(predictions) #print array \n",
    "plt.imshow(imgs[0])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
